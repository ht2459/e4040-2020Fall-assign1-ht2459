{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, cross-entrophy is used to quantify the difference between two probability distributions. Softmax classifier predicts the probabilities for each label. And we have one correct label for each observation as one-hot encoding. $p(x)$ is the actual label, $q(x)$ is the predicted probability by softmax classifier. We sum over all $p(x)$ and $log q(x)$, then we know how far away our prediction from true distribution, which is a good indicator of information loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "Please first describe the difference between multi-class and binary logistic regression; then describe another possible way to derive a multi-class logistic regression classifier from a binary one; finally, illustrate how they work in a deep learning classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-class logistic regression is a general form of binary logistic regression with $y = N * D$  (N = # of observations, D = # of labels) <br>\n",
    "For the binary logistic regression, we calculate the score of positive class. However, we calculate scores for all classes in multi-class logistic regression. <br>\n",
    "We first find the loss function, and calculate the gradient with respect to the parameters. We update the parameters with the learning rate mutiply with the gradient until we minimize the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Relu is one-sided, make it better gradient propagation, which does not suffer much from the vanishing gradient problem.\n",
    "2. Sparse activation, only half of the hidden units are activated, which saves a lot of compute power.\n",
    "3. Efficient Computation: with only addition and multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general procedure is as follows: <br>\n",
    "1. shuffle the dataset\n",
    "2. split the dataset into K groups with equal sized subsamples\n",
    "3. for group in K groups:\n",
    "     1. treat the selected group as test data\n",
    "     2. treat the rest groups as training data\n",
    "     3. fit the model with training data\n",
    "     4. evaluate the model with test data\n",
    "     5. calculate train and test scores\n",
    "4. calculate overall mean train and test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting a smaller learning rate, increasing the hidden_dim size, increasing the num_epoch we can obtain a better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the 10 points bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
